{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f665f0",
   "metadata": {},
   "source": [
    "# Data Visualization \n",
    "# Week 9: Text Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please install NLTK library before use\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db919e6",
   "metadata": {},
   "source": [
    "# 1.Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea51a32",
   "metadata": {},
   "source": [
    "## 1.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a5e5c",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23779cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentence below was taken from https://en.wikipedia.org/wiki/Natural_language_processing. \n",
    "\n",
    "sentence = \"Natural language processing (NLP) is an interdisciplinary subfield of linguistics and computer science. It is primarily concerned with processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376734e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sentence tokenize function \n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing a given text into individual sentences\n",
    "tokenized_sent = sent_tokenize(sentence)\n",
    "\n",
    "for i, sent in enumerate(tokenized_sent):\n",
    "    print(i, ':', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01daaaa",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenize function \n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f775649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing a given text into individual words\n",
    "\n",
    "tokenized_word = word_tokenize(sentence)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(tokenized_word):\n",
    "    print(i, ':', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49f5fa",
   "metadata": {},
   "source": [
    "## 1.2 Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9f40d00",
   "metadata": {},
   "source": [
    "### Lower Casing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the characters in the string to lowercase\n",
    "lowercase_sent = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_tokenized_word = word_tokenize(lowercase_sent)\n",
    "print(lowercase_tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lowercase_tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2263bc1",
   "metadata": {},
   "source": [
    "## 1.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stopwords module\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55133616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "for i in lowercase_tokenized_word:    \n",
    "    if i not in stop_words:\n",
    "         filtered_tokens.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1388640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lowercase_tokenized_word))\n",
    "print(len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3b02e",
   "metadata": {},
   "source": [
    "## 1.4 Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the string module\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of punctuations\n",
    "punct = list(string.punctuation)\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations\n",
    "\n",
    "filtered_tokens_v2 = []\n",
    "\n",
    "for i in filtered_tokens:\n",
    "    if i not in punct:\n",
    "        filtered_tokens_v2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lowercase_tokenized_word))\n",
    "print(len(filtered_tokens))\n",
    "print(len(filtered_tokens_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lowercase_tokenized_word)\n",
    "print()\n",
    "print(filtered_tokens)\n",
    "print()\n",
    "print(filtered_tokens_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447a09f",
   "metadata": {},
   "source": [
    "## 1.5 Stemming\n",
    "#### Stemming: It is a process of transforming a word to its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb600eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PorterStemmer class from the NLTK library\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming on a list of words\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words = []\n",
    "\n",
    "for i in filtered_tokens_v2:     \n",
    "     stemmed_words.append(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original word vs stemmed version\n",
    "for w in filtered_tokens_v2:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemmed_words)\n",
    "print(len(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9ffa2",
   "metadata": {},
   "source": [
    "## 1.6 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the WordNetLemmatizer class from the NLTK library.\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe311535",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized_words)\n",
    "print(len(lemmatized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd5226",
   "metadata": {},
   "source": [
    "## Part-of-Speech(POS) tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform part-of-speech tagging\n",
    "tagged_word = nltk.pos_tag(lemmatized_words)\n",
    "print(tagged_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9c451",
   "metadata": {},
   "source": [
    "# 2. Text Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a70b74",
   "metadata": {},
   "source": [
    "## 2.1 Frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the FreqDist class\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcfe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word = FreqDist(lemmatized_words)\n",
    "print(freq_word)   #Could you please explain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Five most common words\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5900134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequency distribution \n",
    "plt.figure(figsize = (6, 3))\n",
    "freq_word.plot(25, cumulative = False)  # Plot the top 25 most common words.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b21ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word = FreqDist(filtered_tokens_v2)\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bf517",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 3))\n",
    "freq_word.plot(10, cumulative = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f2721",
   "metadata": {},
   "source": [
    "## 2.2 Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please install wordcloud library before use\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f52283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string from the \"lemmatized_words\"\n",
    "filtered_tokens_v3 = \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width = 600, height = 600,\n",
    "                      background_color = 'white',\n",
    "                      min_font_size = 8,\n",
    "                      colormap = 'viridis').generate(filtered_tokens_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display WordCloud \n",
    "#plt.figure(figsize=(5, 5), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f6c22",
   "metadata": {},
   "source": [
    "# 3. Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1fe1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of textblob on sentiment analysis\n",
    "review1 = \"The pizza was delicious. The crust was perfectly crispy, and the toppings were generous. I'll definitely be coming back for more.\"\n",
    "review2 = \"I cannot believe how terrible the food was at this restaurant. The pizza had a cardboard-like crust, and the toppings tasted like they were old and stale. I've never had such a disappointing dining experience in my life. I wouldn't recommend this place to my worst enemy.\"\n",
    "\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob1 = TextBlob(review1)\n",
    "blob2 = TextBlob(review2)\n",
    "\n",
    "# Perform sentiment analysis and print the results\n",
    "print('Review1 :', blob1.sentiment)\n",
    "print('Review2 :', blob2.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85630ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset from https://www.kaggle.com/datasets/jessicali9530/kuc-hackathon-winter-2018\n",
    "df = pd.read_csv('drugsComTest_raw.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922aeb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e12758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame by a string value (\"Depression\") in the \"condition\" column\n",
    "filtered_df = df[df['condition'].str.contains('Depression')]\n",
    "filtered_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to only specific columns\n",
    "filtered_df = filtered_df.loc[:, ['uniqueID', 'drugName', 'condition', 'review']]\n",
    "filtered_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e59c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ac9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML entities\n",
    "    cleaned_text = html.unescape(text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Part-of-speech tagging\n",
    "    pos_tags = pos_tag(lemmatized_words)\n",
    "    \n",
    "    # Filter nouns and adjectives\n",
    "    filtered_words = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('JJ')]\n",
    "    \n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'review' column\n",
    "filtered_df['preprocessed_review'] = filtered_df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d30e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the polarity of the preprocessed reviews using the TextBlob \n",
    "filtered_df['polarity'] = filtered_df['preprocessed_review'].apply(lambda x: (TextBlob(x).sentiment.polarity))\n",
    "filtered_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'get_sentiment' function that converts sentiment scores into sentiment labels. \n",
    "\n",
    "def get_sentiment(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['sentiment'] = filtered_df['polarity'].apply(get_sentiment)\n",
    "filtered_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each polarity value\n",
    "sentiment_counts = filtered_df['sentiment'].value_counts()\n",
    "\n",
    "# Plot a bar chart\n",
    "plt.figure(figsize=(5, 4))\n",
    "sentiment_counts.plot(kind='bar', color='blue')\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d133da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    text = \" \".join(filtered_df[filtered_df['sentiment'] == sentiment]['drugName'])\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Calculate  \n",
    "    freq_dist = FreqDist(words)\n",
    "    \n",
    "    # Plot the frequency distribution\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    freq_dist.plot(30, title=f\"Top 30 words in {sentiment} Sentiment\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e572264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud for each sentiment\n",
    "sentiments = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    text = \" \".join(filtered_df[filtered_df['sentiment'] == sentiment]['drugName'])\n",
    "    \n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                          background_color = 'white',\n",
    "                          min_font_size = 10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize = (5, 5))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(f\"{sentiment} Sentiment\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4113f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
